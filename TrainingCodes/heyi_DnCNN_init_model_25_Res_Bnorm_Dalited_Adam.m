
function net = heyi_DnCNN_init_model_25_Res_Bnorm_Dalited_Adam

lr  = [1 1] ;
lr1 = [1 0];
weightDecay = [1 0];
% Define network 
net.layers = {} ;

% Block 0 : Dilated Convolution+ReLU
%layer 1
net.layers{end+1} = struct('type', 'conv', ...
                           'weights', {{sqrt(1/9)*randn(3,3,1,64,'single'), zeros(64,1,'single')}}, ...
                           'stride', 1, ...
                           'pad', 1, ...
                           'dilate', 1, ...
                           'learningRate',lr, ...
                            'weightDecay',weightDecay, ...
                           'opts',{{}}) ;    
%layer 2
net.layers{end+1} = struct('type', 'relu','leak',0) ;

% Block 1 : 5X Dilated Convolution+Batch Normalization+ReLU
%layer 3
net.layers{end+1} = struct('type', 'conv', ...
                           'weights', {{sqrt(2/(9*64))*randn(3,3,64,64,'single'), zeros(64,1,'single')}}, ...
                           'stride', 1, ...
                           'dilate', 2, ...
                           'learningRate',lr, ...
                           'weightDecay',weightDecay, ...
                           'pad', 2, 'opts', {{}}) ;
%layer 4 BNorm
%layer 5
net.layers{end+1} = struct('type', 'relu','leak',0) ;

%layer 6
net.layers{end+1} = struct('type', 'conv', ...
                           'weights', {{sqrt(2/(9*64))*randn(3,3,64,64,'single'), zeros(64,1,'single')}}, ...
                           'stride', 1, ...
                           'dilate', 3, ...
                           'learningRate',lr, ...
                           'weightDecay',weightDecay, ...
                           'pad', 3, 'opts', {{}}) ;     
%layer 7 BNorm
%layer 8
net.layers{end+1} = struct('type', 'relu','leak',0) ;

%layer 9
net.layers{end+1} = struct('type', 'conv', ...
                           'weights', {{sqrt(2/(9*64))*randn(3,3,64,64,'single'), zeros(64,1,'single')}}, ...
                           'stride', 1, ...
                           'dilate', 4, ...
                           'learningRate',lr, ...
                           'weightDecay',weightDecay, ...
                           'pad', 4, 'opts', {{}}) ;               
%layer 10 BNorm
%layer 11
net.layers{end+1} = struct('type', 'relu','leak',0) ;

%layer 12 
net.layers{end+1} = struct('type', 'conv', ...
                           'weights', {{sqrt(2/(9*64))*randn(3,3,64,64,'single'), zeros(64,1,'single')}}, ...
                           'stride', 1, ...
                           'dilate', 3, ...
                           'learningRate',lr, ...
                           'weightDecay',weightDecay, ...
                           'pad', 3, 'opts', {{}}) ;      
%layer 13 BNorm
%layer 14
net.layers{end+1} = struct('type', 'relu','leak',0) ;

%layer 15
net.layers{end+1} = struct('type', 'conv', ...
                           'weights', {{sqrt(2/(9*64))*randn(3,3,64,64,'single'), zeros(64,1,'single')}}, ...
                           'stride', 1, ...
                           'dilate', 2, ...
                           'learningRate',lr, ...
                           'weightDecay',weightDecay, ...
                           'pad', 2, 'opts', {{}}) ;        
%layer 16 BNorm
%layer 17                                
net.layers{end+1} = struct('type', 'relu','leak',0) ;

%Block 3 Dilated Convolution
%layer 18
net.layers{end+1} = struct('type', 'conv', ...
                           'weights', {{sqrt(2/(9*64))*randn(3,3,64,1,'single'), zeros(1,1,'single')}}, ...
                           'stride', 1, ...
                           'dilate', 1, ...
                           'learningRate',lr, ...
                           'weightDecay',weightDecay, ...
                           'pad', 1, 'opts', {{}}) ;


% add the loss layer                 
net.layers{end+1} = struct('type', 'loss','loss','l2') ;

% add BNorm layers
net = insertBnorm(net, 3) ;
net = insertBnorm(net, 6) ;
net = insertBnorm(net, 9) ;
net = insertBnorm(net, 12) ;
net = insertBnorm(net, 15) ;

% Fill in default values
net = vl_simplenn_tidy(net) ;


% --------------------------------------------------------------------
function net = insertBnorm(net, l)
% --------------------------------------------------------------------
assert(isfield(net.layers{l}, 'weights'));
meanvar  =  [zeros(64,1,'single'), 0.01*ones(64,1,'single')];

layer = struct('type', 'bnorm', ...
        'weights', {{sqrt(2/(9*64))*randn(64,1,'single'), zeros(64,1,'single'),meanvar}}, ...
        'learningRate', [1 1 1], ...
        'weightDecay', [0 0], ...
        'opts', {{}}) ;

net.layers = horzcat(net.layers(1:l), layer, net.layers(l+1:end)) ;










